---
title: "Chapter 3 HW"
author: "Nehemya McCarter-Ribakoff"
date: "28 February 2017"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

```{r setup, include=FALSE}
# DO NOT ALTER CODE IN THIS CHUNK
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

* * *

## Conceptual Questions


#### Exercise 1: Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model.

Null Hypotheses:


Intercept: 

TV: There is no relationship between TV advertising and sales.
    The p-value is below 1%, so we may reject the null hypothesis.
    That is, there is some relationship between TV ads and sales.

radio: There is no relationship between radio advertising and sales.
       The p-value is below 1%, so we may reject the null hypothesis.
       That is, there is some relationship between radio ads and sales.

newspaper: There is not relationship between newspaper advertising and sales
           The p-value is about 86%, which is quite high. Thus, we may retain the            null hypothesis. There is no relationship between newspaper ads and              sales.

#### Exercise 3:  suppose we have a data set with five predictors, X1 = GPA, X2 = IQ, X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between GPA and IQ, and X5 = Interaction between GPA and Gender. The response is starting salary after graduation (in thousands of dollars). suppose we use least squares to fit the model, and get β<sub>0</sub> = 50, β<sub>1</sub> = 20, β<sub>2</sub> = 0.07, β<sub>3</sub> = 35, β<sub>4</sub> = 0.01, β<sub>5</sub> = −10.

Our intercept coefficient is 50; Our average salary is $50,000.

**(a) Which answer is correct, and why?**

**i. For a fixed value of IQ and GPA, males earn more on average
than females.**

If IQ and GPA are fixed, then the only predictor is gender, X3, whose corresponding prediction is β<sub>3</sub> = 35. 

**ii. For a fixed value of IQ and GPA, females earn more on
average than males.**

Same as above, our only predictor is X3, and β<sub>3</sub> = 35.

**iii. For a fixed value of IQ and GPA, males earn more on average
than females provided that the GPA is high enough.**


**iv. For a fixed value of IQ and GPA, females earn more on
average than males provided that the GPA is high enough.**


**(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.**

We simply multiply the predictors to the model's corresponding coefficients.

``` 
(20 * 4.0) + (110 * 0.07) = 87.7
```

We expect her to have a salary of $87,700

**(c) True or false: Since the coefficient for the GPA/IQ interaction
term is very small, there is very little evidence of an interaction
effect. Justify your answer.**

This is true. A coefficient value of 0 means there is no association, and if a value is close, there is little association. A slight positive value suggests there is a weak, positive relationship, but a value so small may be negligible.


* * *

## Applied Questions

#### Exercise 10: This question should be answered using the Carseats data set.

```{r}
library(ISLR)
# Let's get the names so we know what we're doing
names(Carseats)

#Note that unit sales are in $1,000.
```

**(a) Fit a multiple regression model to predict Sales using Price,
Urban, and US.**

```{r}
lm.fit = lm(Carseats$Sales~Carseats$Price+Carseats$Urban+Carseats$US)
lm.fit
```

**(b) Provide an interpretation of each coefficient in the model. Be
careful—some of the variables in the model are qualitative!**

Our Intercept is 13.04 -- when all predictors are averaged, we can expect sales of $13,040

The coefficient of price is -0.05446. As price increases, sales marginally decrease, but this coefficient is so small it is likely negligible.

Stores in urban locations exhibit marginally lower sales. Once again, -0.022 is such a small coefficient, we can probably disregard this.

There is a small, positive relationship between a U.S. store location and an increase in sales.

**(c) Write out the model in equation form, being careful to handle
the qualitative variables properly.**

Sales = β<sub>0</sub> + β<sub>1</sub>Price + β<sub>2</sub>UrbanYes + β<sub>3</sub>USYes + β<sub>4</sub> + β<sub>5</sub>

**(d) For which of the predictors can you reject the null hypothesis
H0 : β<sub>j</sub> = 0?**

We must look at the p-values

```{r}
summary(lm.fit)
```

We can reject the null hypotheses for Price and USYes, as these p-values are both 2<sub>-16</sub>

**(e) On the basis of your response to the previous question, fit a
smaller model that only uses the predictors for which there is
evidence of association with the outcome.**

```{r}
lm.smallFit = lm(Carseats$Sales~Carseats$Price + Carseats$US)
```

**(f) How well do the models in (a) and (e) fit the data?**

We can measure the quality of fit with MSE (for continuous data) and 
```{r}
summary(lm.fit)$sigma #RSE fit
summary(lm.smallFit)$sigma #RSE smallFit
summary(lm.fit)$r.sq #R^2 fit
summary(lm.smallFit)$r.sq #R^2 smallFit
```

The models fit the data about the same, though the original fit has a larger R<sub>2</sub>, so it is a slightly better fit. Both are much closer to 0 than 1, so neither is a very good fit.

**(g) Using the model from (e), obtain 95% confidence intervals for
the coefficient(s).**

```{r}
confint(lm.smallFit, level = 0.95)
```

**(h) Is there evidence of outliers or high leverage observations in the
model from (e)?**


#### Exercise 13: In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to
starting part (a) to ensure consistent results.

**(a) Using the rnorm() function, create a vector, x, containing 100
observations drawn from a N(0, 1) distribution. This represents
a feature, X.**

```{r}
x = rnorm(100, mean = 0, sd = 1)
```

**(b) Using the rnorm() function, create a vector, eps, containing 100
observations drawn from a N(0, 0.25) distribution i.e. a normal
distribution with mean zero and variance 0.25.**

```{r}
eps = rnorm(100, mean = 0, sd = sqrt(0.25))
```

**(c) Using x and eps, generate a vector y according to the model**

<center>**Y = −1+0.5X + ε**</center>


**What is the length of the vector y? What are the values of β<sub>0</sub>
and β<sub>1</sub> in this linear model?**

```{r}
Y = -1 + (0.5 * x) + eps
length(Y)
```

The length of Y is 100. β<sub>0</sub> = -1 and β<sub>1</sub> = 0.5

**(d) Create a scatterplot displaying the relationship between x and
y. Comment on what you observe.**

```{r}
plot(x, Y)
```

There is a general positive relationship between x and Y -- as x increases, Y also increases.

**(e) Fit a least squares linear model to predict y using x. Comment
on the model obtained. How do β<sub>0</sub> and β<sub>1</sub> compare to β<sub>0</sub> and β<sub>1</sub>?**

```{r}
ls.fit = lm(Y~x)
```

**(f) Display the least squares line on the scatterplot obtained in (d).
Draw the population regression line on the plot, in a different
color. Use the legend() command to create an appropriate legend.**

```{r}
plot(x, Y)
abline(ls.fit, col = 3)
abline(-1, 0.5, col = 4)
legend(1,-1.5, legend = c("least squares", "pop. regression"), col=3:4, lwd = 3)
```

**(g) Now fit a polynomial regression model that predicts y using x
and x<sup>2</sup>. Is there evidence that the quadratic term improves the
model fit? Explain your answer.**

```{r}
lm.fit = lm(Y~x + x^2)
summary(lm.fit)
```

Comments here

```{r}
# (e) fit a least squares linear regression model. Comment
# on the model obtained. How do β<sub>0</sub> and β<sub>1</sub> compare to #
# β<sub>0</sub> and β<sub>1</sub>?**

ls.fit = lm(Y~x)
```


Our R<sup>2</sup> has raised significantly from the original model's, so we may say the quadratic term has improved the model fit

**(h) Repeat (a)–(f) after modifying the data generation process in
such a way that there is less noise in the data. The model in (c)
should remain the same. You can do this by decreasing the variance
of the normal distribution used to generate the error term
 in (b). Describe your results.**

sd = sqrt(var), so let's try decreasing sd. we will divide eps' sd by 10
```{r}
x = rnorm(100, mean = 0, sd = 1)
eps = rnorm(100, mean = 0, sd = sqrt(0.25 / 10))
Y = -1 + (0.5 * x) + eps
# (d) create a scatterplot, comment on observations
plot(x, Y)
```

The shape here is decidedly more linear with fewer outliers. It more cloesly represents a simple regression line.

```{r}
# (e) fit a least squares linear regression model. Comment
# on the model obtained. How do β^<sub>0</sub> and β^<sub>1</sub> compare to
# β<sub>0</sub> and β<sub>1</sub>?**

ls.fitLessNoise = lm(Y~x)
summary(ls.fitLessNoise)
```



```{r}
# (f) Display the least squares line on the scatterplot obtained in (d).
# Draw the population regression line on the plot, in a different
# color. Use the legend() command to create an appropriate legend.

plot(x, Y)
abline(ls.fitLessNoise, col = 3)
abline(-1, 0.5, col = 4)
legend(1,-1.5, legend = c("least squares", "pop. regression"), col=3:4, lwd = 3)
```

**(i) Repeat (a)–(f) after modifying the data generation process in
such a way that there is more noise in the data. The model
in (c) should remain the same. You can do this by increasing
the variance of the normal distribution used to generate the
error term  in (b). Describe your results.**

We will multiply eps' sd by 10
```{r}
x = rnorm(100, mean = 0, sd = 1)
eps = rnorm(100, mean = 0, sd = sqrt(2.5))
Y = -1 + (0.5 * x) + eps
# (d) create a scatterplot, comment on observations
plot(x, Y)
```

The data now has virtually no shape. There is no discernible relationship between x and Y

```{r}
# (e) fit a least squares linear regression model. Comment
# on the model obtained. How do β^<sub>0</sub> and β^<sub>1</sub> compare to 
# β<sub>0</sub> and β<sub>1</sub>?**

ls.fitMoreNoise = lm(Y~x)
```



```{r}
# (f) Display the least squares line on the scatterplot obtained in (d).
# Draw the population regression line on the plot, in a different
# color. Use the legend() command to create an appropriate legend.
plot(x, Y)
abline(ls.fitMoreNoise, col = 3)
abline(-1, 0.5, col = 4)
legend(1,-1.8, legend = c("least squares", "pop. regression"), col=3:4, lwd = 3)
```


**(j) What are the confidence intervals for β<sub>0</sub> and β<sub>1</sub> based on the original data set, the noisier data set, and the less noisy data
set? Comment on your results.**

Original data set

```{r}
confint(ls.fit)
```

Less noisy data set

```{r}
confint(ls.fitLessNoise)
```

More noisy data set

```{r}
confint(ls.fitMoreNoise)
```

As we'd expect, the less noisy data set has the most narrow confidence interval, since its data have all been squeezed together. The noisy data set has the widest interval, because its data is scattered everywhere. The original set falls somewhere in between.




* * *

## Teamwork report

<!--
For each team member, write their name, whether they attended the lab session, 
whether they were the author for this week, and their contribution to the lab 
report (as a percentage). The percentages should add up to 100.
-->

| Team member             | Conceptual | Applied  | Contribution % |
|:------------------------|:----------:|:--------:|---------------:|
| Nehemya                 |    Yes     |   Yes    | 100%           |
| Total                   |            |          | 100%           |