---
title: "Chapter 6 HW"
author: "Nehemya McCarter-Ribakoff"
date: "13 April 2017"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

```{r setup, include=FALSE}
# DO NOT ALTER CODE IN THIS CHUNK
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
```

* * *
## Conceptual Questions


#### Exercise 1: We perform best subset, forward stepwise, and backward stepwise
selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2,...,p predictors. Explain your answers:

##### (a) Which of the three models with k predictors has the smallest training RSS?

A naive best subset selection approach will select the model with the smallest training RSS. Since RSS decrease monotonically w.r.t the number of predictors, this model will be the one with p + 1 predictors. This is why it is called naive, and the model's low training RSS will not hold up against test data. Best subset selection is also computationally expensive.

##### (b) Which of the three models with k predictors has the smallest test RSS?

This is the ultimate goal, so naturally, there is no straightforward answer to which of these models will have the smallest test RSS. The best we can do is estimate the test error directly or indirectly.

##### (c) True or False:

###### i. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by forward stepwise selection.

True

###### ii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by backward stepwise selection.

True

###### iii. The predictors in the k-variable model identified by backward stepwise are a subset of the predictors in the (k + 1)-variable model identified by forward stepwise selection.

True

###### iv. The predictors in the k-variable model identified by forward stepwise are a subset of the predictors in the (k+1)-variable model identified by backward stepwise selection.

False. Backward stepwise selection is not possible when n < p because the whole model cannot be fit. Forward stepwise selection does not have this problem.

###### v. The predictors in the k-variable model identified by best subset are a subset of the predictors in the (k + 1)-variable model identified by best subset selection.

True

#### Exercise 2: For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer.

##### (a) The lasso, relative to least squares, is:

###### i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

Incorrect. Like ridge regression, the lasso decreases flexibility as 位 increases. The bias-variance tradeoff described here is correct: the lasso will take a small increast in bias for a larger decrease in variance.

###### ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Incorrect. The lasso has less flexibility, and as mentioned above, the bias-variance tradeoff is reversed from the one mentioned here.

###### iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

Correct. The lasso causes a decrease in flexibility, which means bias to the model will be somewhat higher, but this is acceptable if the bias increase is less than the consequent decrease in variance. Additionally, the lasso may drop some predictors entirely, resulting in a simpler model.

###### iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Incorrect. The lasso aims to decrease variance for a slight increase in bias. This has the relationship mixed up.

##### (b) Repeat (a) for ridge regression relative to least squares.

###### i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

Incorrect. Ridge regression uses a multiplicative term 位 that causes a decrease in the model's flexibility as it increases. This gives the model a slight increase in bias for a much larger decrease in variance.

###### ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Incorrect, once again, ridge regression results in _less_ flexibility. Moreover, less flexibility translates into less variance, since both refer to far a model will deviate from its pattern (e.g., linear) in order to more closely fit any given set of training data. Since variance is a measure of how much a model's shape changes with a given training set, ridge regression's decrease in flexibility means a decrease in variance.

###### iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

Correct. Ridge regression causes a decrease in flexibility, which means bias to the model will be somewhat higher, but this is acceptable if the bias increase is less than the consequent decrease in variance.

###### iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Incorrect. Ridge regression aims to decrease variance for a slight increase in bias. This has the relationship mixed up.

##### (c) Repeat (a) for non-linear methods relative to least squares.

###### i. More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

Incorrect. A non-linear method will certainly have more flexibility than least squares, but this added flexibility will result in an increase in variance and a derease in bias.

###### ii. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Correct. As stated above, non-linear methods are not as rigid as least squares. Their flexibility will provide a lower bias, but higher variance. If the increase in variance is less than the decrease in bias, then it is a better approach than least squares.

###### iii. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance.

Incorrect. Non-linear models are more flexible.

###### iv. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias.

Incorrect. Non-linear models are more flexible, though the bias-variance tradeoff described here is an accurate for non-linear models.


* * *


## Applied Questions

#### Exercise 9: In this exercise, we will predict the number of applications received using the other variables in the College data set.

##### (a) Split the data set into a training set and a test set.

```{r}
library(ISLR)
library(caTools)
attach(College)

set.seed(1)
apps.true = sample.split(College$Apps, 2/3)
set.train = subset(College, apps.true == TRUE)
set.test = subset(College, apps.true == FALSE)
```

##### (b) Fit a linear model using least squares on the training set, and report the test error obtained.

```{r}
College.full = na.omit(College)
lm.fit = lm(Apps ~., set.train)
summary(lm.fit)
```

Test error: 
##### (c) Fit a ridge regression model on the training set, with 位 chosen by cross-validation. Report the test error obtained.

```{r}
library(glmnet)
College = na.omit(College)
X.train = model.matrix(Apps~., College)[,-1]
X.test = model.matrix(Apps~., College)[,-1]
Y.train = set.train$Apps
grid = 10 ^ seq(10, -2, length=100)

mod.ridge = cv.glmnet(X.train, Y.train, alpha = 0, lambda=grid)
best.lambda = mod.ridge$lambda.1se
best.lambda
ridge.pred = predict(mod.ridge, newx=X.test, s=best.lambda)
ridge.mse = mean((test$Apps - ridge.pred)^2)
ridge.mse
```

##### (d) Fit a lasso model on the training set, with 位 chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.

```{r}

```

##### (e) Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.

```{r}

```

##### (f) Fit a PLS model on the training set, with M chosen by crossvalidation.
Report the test error obtained, along with the value of M selected by cross-validation.

```{r}

```

##### (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

```{r}

```

* * *

## Teamwork report

<!--
For each team member, write their name, whether they attended the lab session, 
whether they were the author for this week, and their contribution to the lab 
report (as a percentage). The percentages should add up to 100.
-->

| Team member             | Conceptual | Applied  | Contribution % |
|:------------------------|:----------:|:--------:|---------------:|
| Nehemya                 |    Yes     |   Yes    | 100%           |
| Total                   |            |          | 100%           |